\chapter{Methods} \label{methods}
This Chapter contains a thoroughly description with rationales of the research methods adopted by this thesis work. Firstly, a characterization of the case is provided. It contains the description of the settings, the case itself and the units of analysis. Subsequently, the focus shifts to the methods and sources through which the data has been collected. For every different source the text describes where applicable use cases and rationale. Finally, the chapter ends with an explanation of the data analysis methods used. 


%
%
% START CASE DESCRIPTION
%
%
\section{Case Study design}	\label{sec:case-description}
Yin \cite{case_study_guide} defines case studies as \textit{``an empirical enquiry that investigates a contemporary phenomenon within its real-life context, especially when the boundaries between phenomenon and context are not clearly evident.''}
Furthermore, as reported by Runeson et al. \cite[p.~3]{case_study_software_engineering}, \textit{``Case studies do not generate the same results on, for example, causal relationships, as controlled experiments do, but they provide a deeper understanding of the phenomena under study''}. Therefore, considering the setting of this study and its aim, this research methodology is the most suitable to answer the research questions listed in Section \ref{sec:introduction}.

Precisely, this study will use the \textit{multiple, holistic case study} design defined by Yin \cite{case_study_guide} and displayed in Fig. \ref{fig:case-study-types}. Furthermore, given the weak body of knowledge (see section \ref{sec:related_work}) this Thesis also includes the exploratory variant. This choice is supported by \cite[p.~19]{case_study_software_engineering}: \textit{``.For exploratory research questions, the case study strategy is a perfect match''}.

However, to counter the criticisms moved to this methodology \cite[p.~4]{case_study_software_engineering}, a rigorous approach is needed. For this reason, the guidelines expressed by the Runeson et al.\ \cite{case_study_software_engineering} are taken into consideration thoroughly. They specifically target the Software Engineering field providing a detailed checklist contained in Appendix \ref{checklist_for_case_studies} with embedded links to the relevant parts of this report. Additionally, the case study reference proposed by Yin \cite{case_study_guide}, which doesn't constrain the scope to a precise field, is used.


For adequately characterize the Case under study Yin \cite{case_study_guide}, suggests to describe 1) the context in which the Case is, 2) the Case itself, and 3) the Units of Analysis. However, as Fig. \ref{fig:case-study-types} shows, when applying the holistic design cases and units of analysis are equivalent. Therefore, the following sections report the generic context and cases with their particular contexts.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=\textwidth]{figure/yin-case-studies.pdf}
    \caption{Yin \cite{case_study_guide} possible case studies configuration.}
    \label{fig:case-study-types}
\end{figure}


\subsection{The generic context}

This study took place in the Swedish office of the Company, a fictional name used to indicate a real entity that globally counts more than 3000 employees scattered in several offices. Its core business is licensing software products for managing personnel and equipment in the context of Transportation. Their portfolio offers different solutions to cover several business needs in terms of short or long term planning, personnel optimization, etc.

Internally, teams use Scrum for managing their development activities. They have considerable freedom in choosing sprint length, and the Definition of Done, which is built atop of a minimum one provided by the parent company. Teams are cross-functional and hence contain people with different areas of expertise; they also are self-contained meaning that they should carry out all software related activities independently.

This last fact is of great relevance to this study. In fact, all testing activities are carried on separately, following the minimum guidelines expressed in the Definition of Done. However, this lenient artifact causes teams to use different testing strategies for complying with the testing pipeline. Therefore, there is a chance to analyze TD accruing in various projects. 

As shown in Fig. \ref{fig:testing_pipeline}, this pipeline is composed of four levels (lower to higher): unit tests, components tests, system tests, end to end tests. These levels do not differentiate between functional and quality (non-functional) testing types as these can take place at any level. 

Notably, unit tests are carried on by developers alongside development activities and aim to test the System at function and class level. Therefore, there is a high number of them and tend to be fast to execute and relatively simple. At this level, developers follow the same guidelines specified for the actual source code, with the sole exception that, according to the minimum definition of done, they are not reviewed. Above in the hierarchy, there are component tests that target the interaction between macro software entities and aim to ensure that the sub-parts of the system deliver the desired functionality. At this level, tests target the exposed APIs and not the internal mechanics of the source code. The next level in the pipeline, System level tests, aims to assure that user functionality and business processes of a single system comply with the specification. These tests execute at user interface level with realistic data sets and possibly data volume. Finally, End to End tests are similar in concept and execution to System tests, but they test the interaction of two or more complete systems. Furthermore, these levels can be further divided into functional and non-functional tests, which respectively target functionality and quality attributes of the system.

Finally, the two middle layers of the pipeline contain test that can be semantically divided into two groups: smoke tests and regression tests. The former group contains simpler tests that control the behavior of macro functionalities, e.g.\ a window is displayed if the new window button is pressed. The latter group, instead, includes tests that represent use-case scenarios and hence more detailed and complicated. The following sub-section provides additional information on the topic.

\todo{Improve figure \ref{fig:testing_pipeline} readability}
\todo{Extract the image from the testing chapter document}

\begin{figure}[hbt]
    \centering
    \includegraphics[width=\textwidth]{figure/testing_pipeline.pdf}
    \caption{Testing pipeline in use at the Company}
    \label{fig:testing_pipeline}
\end{figure}

\subsection{Cases definition and units of Analysis}

Among the ongoing projects at the Company, this study will inspect the accretion of TD in projects using Hewlett Packard's Unified Functional Tests (UFT) to perform user interface tests.

To properly framing the inquiry the chosen \textit{Cases} are the projects that use Hewlett Packard UFT (Unified Functional Test) for GUI testing (formerly known as QTP) both for smoke testing and for regression testing. This tool allows testers to mix Property Based GUI testing with Image Recognition GUI testing. Within these projects I will evaluate the maintenance activities that target the two higher testing levels of the functional part of the mentioned pipeline. Specifically how much extra maintenance derives from the use of bad code practices used in GUI test scripts. With that said, the chosen \textit{Unit of Analysis} are the such maintenance activities. 
    
\todo{Divide thesis in case A (manpower) and B (atrium)}
%
%
% END CASE DESCRITPION
%
%

%
%
% START DATA GATHERING
%
%
\section{Data Gathering}
As recommended by \cite{case_study_guide,case_study_software_engineering}, data has been gathered from various sources in order to increase Data Triangulation and hence improve the reliability of the study. These sources are: 1) informal interviews (a.k.a.\ coffee machine / water bowl interviews), 2) the company´s forum, 3) the company´s issue tracker and backlog, 4) the repositories of the relevant project, and 5) semi-structured interviews.

Finally, considering the extensive amount of data that has been generated, it has proven impossible to insert the raw results in the report. Instead, it is available on-line at \href{http://???/}. However, readers should be aware that the raw data was anonymized and every sensitive information was also removed. 

\todo{Insert link to thesis database}
\todo{Mention that data is 3rd stage}
\todo{Mention that I used both quantitative and qualitative sources for improving reliability}

\subsection{Informal Interviews}
As stated by Milberg and Strang \cite{informal_interview} researchers should strive to attain qualitative data from various sources in order to gain a well-rounded knowledge of the phenomenon under study. For this reason, this study also collected information from Informal Interviews.

This source helped defining the scope of the inquiry in the first stages of the study by identifying at very high-level main problems and triggering causes. However, due to the lack of repeatability associated with this method, such data has not been used for sensitive parts of the study like theory validation.

In fact, frequency, location, and interviewees were random. Furthermore, as highlighted by McNamara \cite{general_guidelines_for_interview}, this kind of inquiry is not steered by schemes, but instead affected by the interaction between participants. Moreover, they occurred during lunch and coffee breaks whenever possible, and bootstrapped by current issues encountered by the interviewee. Apparently a not controlled environment. However, their importance during the first phases of this Thesis is clear since they focused the attention on important parts, sources, and documentation.

\subsection{Mining of The Forum} \label{mining_forum}
In order to enable Knowledge Sharing the company adopted Confluence, a collaborative software / forum developed and maintained by Atlassian. It allows technical and non-technical employees to share procedures, ideas, and comments in an asynchronous fashion, and, for this reason, this source is valuable since it contains numerous information, which can shed light on the problems this Thesis aim to study.

However, considering the magnitude of the available data, it was important to ensure that the mining did not miss any information. Therefore, it has been performed through a systematic approach using the embedded search engine provided by the tool and two manual filtering steps. In fact, the system contains data covering several years and include several thousand discussions threads and many more answers. 

The keywords entered in the search engine were included in double quotes in order to enable the \textit{exact match} feature. This choice was needed to limit the number of results to a manageable size. For instance, \textit{test maintenance} yielded 8467 results whereas \textit{"test maintenance"} 28. Subsequently, the first coarser temporal based filter followed. It eliminated hits older than one year. Next, the second finer filter based on the relevance was used. Finally, the remaining set of posts has been carefully read, and pertinent ones have been anonymized and transcribed in the Thesis database. Table \ref{tab:confluenceMiningResults} displays the results obtained at each step for each keywords.

This paragraph reports the rationale connected with the first filtering activity. The temporal time frame is narrow for two reasons. Firstly, the test-ware stack changed during last year (2014) and hence the findings could be biased by information not relevant or tied to the old technologies. Secondly, the mortality of the resources has been taken into account. In fact, the Company greatly relies on consultants, and their turnover is shorter than typical employees. 

\subsection{Mining of The Issue Tracker / Backlogs} \label{mining_issue_tracker}
The Company uses an all in one solution for managing issues and their Agile Environments: JIRA and JIRA Agile Plugin. These tools are also supported by Atlassian. Similarly to the forum, this source contains a high number of entries that have been mined to provide insights regarding the problems under analysis.

This activity has been carried on firstly with a parsing script \footnote{\href{https://github.com/MMMarcy/JiraMiner4Thesis}{https://github.com/MMMarcy/JiraMiner4Thesis}} that, after parsing the JSON file generated through the JIRA Rest APIs \footnote{\href{https://developer.atlassian.com/jiradev/api-reference/jira-rest-apis}{https://developer.atlassian.com/jiradev/api-reference/jira-rest-apis}}, created a spreadsheet with the relevant data of choice.

The filtering process used the JQL query language, which is supported natively by the APIs mentioned above. The data clause targeted by the query was \textit{text}, which is a macro field that includes issue's name, description, summary, and comments. Regarding the keywords used, they were tested in a trial  and error fashion. However, the process showed that the query using a boolean or with \textit{``UFT''} and \textit{``QTP''} yielded the best combination of relevant results.

The spreadsheet contains some statistics not provided by the APIs, but thanks to their relevance, the script uses arithmetic operations to generate them. For instance, the column \textit{´´Effective time (h)``} uses the \textit{´´Modified on``} and \textit{´´Closed on``} to generate an approximate time through subtraction. Differently, some columns have been manually formed since an automatic analysis would require text comprehension that is out of the scope of this Thesis. Table \ref{tab:spreadsheet_description} shows all the data fields included, provides a rationale, and finally shows an example record for every column.


\begin{table}[htb]
			\centering
			\renewcommand{\arraystretch}{1.2}
			\tabulinesep=1.2mm
            \caption[Original spreadsheet description]{Description of the original spreadsheet created by the process described in section \ref{mining_issue_tracker}}
            \label{tab:spreadsheet_description}
            
			\begin{tabu} to \textwidth {|X|X[4]|X|}
			    
				\hline
				\textbf{Col. name} & \textbf{Description} & \textbf{Example} \\ \hline
				
				Key & This entry is composed by two parts: Project name and issue number. It was used to keep a link between projects under study and data gathered & PRJONE-001 \\ \hline
				
				Priority & Indicates the priority assigned to the issues by internal stakeholders after the evaluation phase of the sprint. A lower number means greater priority. & 3 \\ \hline
				
				Estimate time & Shows the effort estimation performed by the developers of the team & 2.5 \\ \hline
				
				Effective time & Shows the effective time it took to complete the issue from when the status was updated & 5 \\ \hline
				
				Opened on & Date and time of when the issue was opened. & Tue Jan 07 09:39:59 CET 2014 \\ \hline
				
				Updated on & Date and time of when the work on the issue started & Thu Jan 22 10:41:27 CET 2015  \\ \hline
				
				Closed on & Date and time of when the issue was closed & Wed Jan 21 14:58:25 CET 2015 \\ \hline
				
				Description & Field that contains description, summary and comments related to the issue. & The process pipeline break when... \\ \hline
				
			\end{tabu}		
		\end{table}

\todo{Fix the script output with dates that are parseable by excel :( }



For future studies an version of the spreadsheet stripped of all sensitive information is available at \inote{Link of spreadsheet}. The process removed all these column that were intellectual properties of the Company, e.g.\ Project name has been replaced with a cardinal number in order to allow a per-project analysis.


\subsection{Analysis of The Repositories} \label{analysis_of_the_repos}
The company extensively uses \textit{Concurrent Versions System}. At the time of writing their system contains 870 repositories of various size belonging to 20 different projects in a many-to-many relationship. The company also deployed two different tools to browse such repositories with ease: \textit{HgWeb} and \textit{Crucible + FishEye}. They are migrating to the latter since it also allows asynchronous code reviews and integrates seamlessly with the Issue Tracker and the Forum as part of the suite developed by Atlassian.

Given the amount of data contained in this archival source, the data coming from other sources was used to select the subset of repositories that better suits the goal of this Thesis. I.e.\ a selected number of repositories targeting test code that are currently under active development or maintenance.

\todo{Finishing this section describing what I did with the commits when I will perform the mining}

\subsection{Semi-Structured Interviews} \label{semi-structured_interviews}
During this study total of \inote{Number of Interviews here} semi-structured interviews was conducted; they follow the guidelines expressed in \cite{interview_guideline}. This interview format allows a sufficient degree of flexibility necessary to adapt the inquiry while maintaining the scope constrained.

\todo{Finish the subsection describing settings, who has been interviewed and such
}



%
%
% END DATA GATHERING
%
%


%
%
% START DATA INTERPRETATION
%
%
\section{Interpretation of the Data} \label{data_interpretation}

The interpretation of the data plays a key role in experimental studies like this one. Furthermore, analyzing both qualitative sources (e.g.\ Forum) and quantitative ones (e.g.\ Repositories) requires an approach that successfully melds together these different sources. For this reason, I believe that using \textit{Coding} \cite{qualitative_inquiry} for qualitative inquiry and \textit{Pattern matching} \cite{case_study_guide} for a quantitative data is the best approach.

The approach followed in this study relies on both methods concurrently. In fact, I used the coding practices in the first part of the study, aiming to extrapolate recurrent patterns and problems from different sources which in turn allowed me to narrow the scope. Subsequently, through pattern matching I create a first theory draft that has been revised with more qualitative inquiry. Finally, through semi-structured interviews, I validated such theory with the help of practitioners.

The two following subsections thoroughly describe the mentioned approaches, however the results are reported in Chapter \ref{study_results}.

\subsection{Coding}
In this study I followed the three coding techniques described by Corbin and Strauss \cite{coding_guidelines}. Namely 1) open coding, 2) axial coding, and 3) selective coding. Furthermore, these tasks were aided by \href{http://www.qsrinternational.com/products_nvivo.aspx}{NVivo 10 for Windows} which allow to manage the coding process with ease.

Open coding is used to analytically break down data and its purpose is to gain new insight about the phenomenon by using non-conventional interpretation criteria. During this process the data fragments are conceptually labeled and similar ones tied together. This creates categories and sub-categories that can be broken down in even finer items such as properties and characteristics. This intensive fracturing of the data forces the researcher to examine preexisting ideas against a very detailed representation of data itself and hence it eliminates subjectivity and bias.

In axial coding the researcher test the connection between categories and sub-categories against the data by analyzing connections, consequences, and conditions. Furthermore, during this step there is a further development of the categories created during open coding. It is recommended to alternate the collection and analysis of the data throughout the study since the analysis phase can guide with more precision the data gathering activities.

Selective coding is the stage in which categories are connected to a \textit{core category} that represents the phenomenon under study. This core category can be identified by asking questions like: \textit{``what does all the action/interaction seem to be about?''}, \textit{``How can I explain all the variation I see among categories?''}, etc.\ \cite{coding_guidelines}. It is obvious that this stage must takes place in the later phases of the study.

Finally, I am going to describe how these coding phases have been carried out during the study. The first open coding iteration took place after six weeks since prior to that the data was too scarce. Afterwards every time one of the data sources described before was exploited, I went through the open coding and axial coding as recommended and hence  I continuously improved the categories previously created. Finally, I performed the Selective coding in \tocite{Enter week number when the selective coding has been performed}.


\subsection{Pattern matching}
Yin \cite{case_study_guide} and Trochim \cite{pattern_matching} consider Pattern matching as one of the best approaches to meld together existing theories with the data gathered within a study. Furthermore, it eases the evaluation of rival theories since a it allows to rule them out thanks to the fitness with the discovered pattern \cite{pattern_matching}.

In this Thesis, the pattern was extrapolated from the Repositories and matched against quantitative and qualitative models provided by the literature. For obvious reasons, in the latter case the inference conducted has been solely inductive, meaning that the findings associated are not corroborated by statistical weight and hence might lack external validity.

\todo{Select wich pattern matching I will use (need to read the paper more carefully)}

Trochim \cite{pattern_matching} highlights several pitfalls that must be considered when using this technique; otherwise the power of this method is greatly hindered. 

Firstly, the conceptualization of the model must be appropriate, e.g.\ using formulae to describe a purely qualitative model is a bad practice since constants derive from the researcher best guesses.

Secondly, the level of generalization of the formulated pattern must match the purpose of the study and the study and the desired level of generalization.

Third, pattern formulation tend to be more effective when performed without prior knowledge of the observed pattern. In fact, the researcher judgment could be biased by this prior knowledge. Fact that inflated the threats to reliability and internal validity.

Next, the holistic perspective that researchers tend to use while performing pattern matching could exclude relevant details. This specially applies when pattern are extracted from a set of metrics or measures.

Finally, when measuring the fitness of patterns using statistical methods their analytical accuracy with respect to the context in which are embedded must be considered in order to avoid false positive results.


\todo{Expand these risks and measures taken to avoid them in validity threats section}