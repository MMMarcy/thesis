\chapter{Methods} \label{methods}
This Chapter contains a thoroughly description, together with rationales, of the research methods adopted by this thesis work. Firstly, a characterization of the case is provided. It contains the description of the settings, the cases and the units of analysis. Subsequently, the focus shifts to the methods and sources through which the data has been collected. For every different source the text describes applicable use cases and rationales. Finally, the chapter ends with an explanation of the data analysis methods used.


%
%
% START CASE DESCRIPTION
%
%
\section{Case Study design}	\label{sec:case-description}
Yin \cite{case_study_guide} defines case studies as \textit{``an empirical enquiry that investigates a contemporary phenomenon within its real-life context, especially when the boundaries between phenomenon and context are not clearly evident.''}
Furthermore, as reported by Runeson et al.\ \cite[p.~3]{case_study_software_engineering}, \textit{``Case studies do not generate the same results on, for example, causal relationships, as controlled experiments do, but they provide a deeper understanding of the phenomena under study''}. Therefore, considering the real life settings of this study and its aim, this research methodology is the most suitable to answer the research questions listed in Section \ref{sec:introduction}.

Precisely, this study will use the \textit{multiple, holistic case study design} defined by Yin \cite{case_study_guide} and displayed in Fig.\ \ref{fig:case-study-types}. Furthermore, given the weak body of knowledge (see section \ref{sec:related_work}) this Thesis also includes the exploratory variant. This choice is supported by \cite[p.~19]{case_study_software_engineering}: \textit{``For exploratory research questions, the case study strategy is a perfect match''}.

However, to counter the criticisms moved to this methodology \cite[p.~4]{case_study_software_engineering}, a rigorous approach is needed. For this reason, the guidelines expressed by the Runeson et al.\ \cite{case_study_software_engineering} are taken into consideration thoroughly. They specifically target the Software Engineering field providing a detailed checklist contained in Appendix \ref{checklist_for_case_studies} for clarity. Additionally, this Thesis work attains from the case study reference proposed by Yin \cite{case_study_guide}, which doesn't constrain the scope to a precise field.

Moreover, for adequately characterize the Case under study, Yin \cite{case_study_guide}, suggests to describe 1) the context in which the case is, 2) the case itself, and 3) the units of analysis. However, as Fig. \ref{fig:case-study-types} shows, when applying the holistic design cases and units of analysis are equivalent. Therefore, the following sections report the generic context and cases with their particular contexts.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=\textwidth]{figure/yin-case-studies.pdf}
    \caption{Yin \cite{case_study_guide} possible case studies configuration.}
    \label{fig:case-study-types}
\end{figure}


\subsection{The generic context}

This study took place in the Swedish office of the Company, a fictional name used to indicate a real entity that globally counts more than 3000 employees scattered in several offices. Its core business is licensing software products for managing personnel and equipment in the context of avionic transportation. Their portfolio offers different solutions to cover several business needs in terms of short or long term planning, personnel optimization, etc.

Internally, teams use Scrum for managing their development activities. They have considerable freedom in choosing sprint length, and the Definition of Done, which is built atop of a minimum one provided by the parent company. Teams are cross-functional and hence contain people with different areas of expertise; they also are self-contained meaning that they should carry out all software related activities independently.

This last fact is of great relevance to this study. In fact, all testing activities are carried on separately, following the minimum guidelines expressed in the Definition of Done. However, this lenient artifact causes teams to use different testing strategies for complying with the testing pipeline. Therefore, there is a chance to analyze TD accruing in various projects.

As shown in Fig.\ \ref{fig:testing_pipeline}, this pipeline is composed of four levels (lower to higher): unit tests, components tests, system tests, end to end tests. These levels do not differentiate between functional and quality (non-functional) testing types as these can take place at any level.

Notably, unit tests are carried on by developers alongside development activities and aim to test the System at function and class level. Therefore, there is a high number of them and tend to be fast to execute and relatively simple. At this level, developers follow the same guidelines specified for the actual source code, with the sole exception that, according to the minimum definition of done, they are not reviewed. Above in the hierarchy, there are component tests that target the interaction between macro software entities and aim to ensure that the sub-parts of the system deliver the desired functionality. At this level, tests target the exposed APIs and not the internal mechanics of the source code. The next level in the pipeline, System level tests, aims to assure that user functionality and business processes of a single system comply with the specification. These tests execute at user interface level with realistic data sets and possibly data volume. Finally, End to End tests are similar in concept and execution to system tests, but they test the interaction of two or more complete systems. Furthermore, these levels can be further divided into functional and non-functional tests, which respectively target functionality and quality attributes of the system.

Finally, the two middle layers of the pipeline contain test that can be semantically divided into two groups: smoke tests and regression tests. The former group contains simpler tests that control the behavior of macro functionalities, e.g.\ a window is displayed if the new window button is pressed. The latter group, instead, includes tests that represent use-case scenarios and therefore are more detailed and complicated. The following sub-section provides additional information on the topic.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=\textwidth]{figure/testing_pipeline.pdf}
    \caption{Testing pipeline in use at the Company}
    \label{fig:testing_pipeline}
\end{figure}

\subsection{Cases definition and units of Analysis} \label{sec:cases_definition}

Among the ongoing projects at the Company, this study will inspect the accretion of TD in projects using Hewlett Packard's Unified Functional Tests (UFT) to test user interfaces. Precisely, there are three projects meeting this requirement and will be called Project A, B, and C in the rest of the document.

All projects have associated multiple repositories; some for smoke testing and the remaining for regression testing. Furthermore, there is a shared repository that contains common methods and functions used by test scripts. For this reason, considering the substantial difference in purpose of this repository it is considered separately. Therefore, the label \textit{Project D} is used to characterize it as an abstract but separate entity that does not map to any real active project.


\subsubsection{Project A}
This project is the new solution developed to replace the legacy Project B. It is Java based and the client runs on the Windows operating system. The back-end runs on Linux and, since it is headless, i.e.\ without user interface, it was not included in this study. The user interface is built on top of the SWING framework with the Windows' Look and Feel provided by the Java Virtual Machine.

\subsubsection{Project B}
Project B is a tool used for registering and handling passengers. It register who is on-board of which airplane and so forth. This tool is less customizable than project A, and it is built on a dated technology stack. However, it is very complex and it still is the reference system until Project A reaches feature parity. The client and server are monolithic and the client runs as a remote Unix X-sessions from other operative systems.

\subsubsection{Project C}
Project C consists of a highly customizable software used to manage crew personnel both in the short and long term handling both law requirements and crews' preferences in terms of Holiday, destinations, and routes. The client runs on Windows and it is realized with Java. As for project A, the headless back-end was not included in this study for obvious reasons.

\subsubsection{Project D}
As mentioned before, Project D consists of two repositories that contain generic functions used by test suites. These tie UFT tests with the Windows operating system. They allow to kill and spawn a process, create and read files, etc. Furthermore, they also create skeleton for the report engines; they are used to create summaries of test runs in different formats such as HTML, XML, etc.

%
%
% END CASE DESCRITPION
%
%

%
%
% START DATA GATHERING
%
%
\section{Data Gathering}
As recommended by \cite{case_study_guide,case_study_software_engineering}, data has been gathered from various sources in order to increase Data Triangulation and hence improve the reliability of the study. These sources are: 1) informal interviews (a.k.a.\ coffee machine / water bowl interviews), 2) the company´s forum, 3) the company´s issue tracker and backlog, 4) the repositories of the relevant project, and 5) semi-structured interviews.

\subsection{Informal Interviews}
As stated by Milberg and Strang \cite{informal_interview} researchers should strive to attain qualitative data from various sources in order to gain a well-rounded knowledge of the phenomenon under study. For this reason, this study also collected information from informal interviews.

This source helped defining the scope of the inquiry in the first stages of the study by identifying at very high-level main problems and triggering causes. However, due to the lack of repeatability associated with this method, such data has not been used for sensitive parts of the study like theory validation.

In fact, frequency, location, and interviewees were random. Furthermore, as highlighted by McNamara \cite{general_guidelines_for_interview}, this kind of inquiry is not steered by schemes, but instead affected by the interaction between participants. Moreover, they occurred during lunch and coffee breaks whenever possible, and bootstrapped by current issues encountered by the interviewee. Apparently a not controlled environment. However, their importance during the first phases of this Thesis is clear since they focused the attention on important parts, sources, and documentation.

\subsection{Mining of The Forum} \label{mining_forum}
In order to enable knowledge sharing the company adopted Confluence, a collaborative software / forum developed and maintained by Atlassian\texttrademark\. It allows technical and non-technical employees to share procedures, ideas, and comments in an asynchronous fashion, and, for this reason, this source is valuable since it contains numerous information, which can shed light on the problems this Thesis aim to study.

However, considering the magnitude of the available data, it was important to ensure that the mining did not miss any information. Therefore, it has been performed through a systematic approach using the embedded search engine provided by the tool and two manual filtering steps. In fact, the system contains data covering several years and include several thousand discussions threads and many more answers.

The keywords entered in the search engine were included in double quotes in order to enable the \textit{exact match} feature. This choice was needed to limit the number of results to a manageable size. For instance, \textit{test maintenance} yielded 8467 results whereas \textit{"test maintenance"} 28. Subsequently, the first coarser temporal based filter followed. It eliminated hits older than one year. Next, the second finer filter based on the relevance was used. Finally, the remaining set of posts has been carefully read, and pertinent ones have been anonymized and transcribed in the Thesis database.

This paragraph reports the rationale connected with the first filtering activity. The temporal time frame is narrow for two reasons. Firstly, the testware stack changed during year 2014 and hence the findings could be biased by information not relevant or tied to the old technologies. Secondly, the mortality of the resources has been taken into account. In fact, the Company greatly relies on consultants, and their turnover is shorter than typical employees.

\subsection{Mining of The issue tracker / backlogs} \label{mining_issue_tracker}
The Company uses an all in one solution for managing issues and their Agile Environments: JIRA and JIRA Agile Plugin. These tools are also supported by Atlassian. Similarly to the forum, this source contains a high number of entries that have been mined to provide insights regarding the problems under analysis.

This activity has been carried on firstly with a parsing script \footnote{\href{https://goo.gl/Jk4hQv}{https://goo.gl/Jk4hQv}} that, after parsing the JSON file generated through the JIRA Rest APIs \footnote{\href{https://goo.gl/16dy70}{https://goo.gl/16dy70}}, created a spreadsheet with the relevant data of choice.

The filtering process used the JQL query language, which is supported natively by the APIs mentioned above. The data clause targeted by the query was \textit{text}, which is a macro field that includes issue's name, description, summary, and comments. Regarding the keywords used, they were tested in a trial  and error fashion. However, the process showed that the query using a boolean or with \textit{``UFT''} and \textit{``QTP''} yielded the best combination of relevant results.

The spreadsheet contains some statistics not provided by the APIs, but thanks to their relevance, the script uses arithmetic operations to generate them. For instance, the column \textit{``Effective time (h)''} uses the \textit{``Modified on''} and \textit{``Closed on''} to generate an approximate time through subtraction. Differently, some columns have been manually formed since an automatic analysis would require text comprehension that is out of the scope of this Thesis. Table \ref{tab:spreadsheet_description} shows all the data fields included, provides a rationale, and finally shows an example record for every column.



\begin{table}[!htb]
			\centering
			\renewcommand{\arraystretch}{1.2}
			\tabulinesep=1.2mm
            \caption[Original spreadsheet description]{Description of the original spreadsheet created by the process described in section \ref{mining_issue_tracker}}
            \label{tab:spreadsheet_description}

			\begin{tabu} to \textwidth {|X|X[4]|X|}

				\hline
				\textbf{Col. name} & \textbf{Description} & \textbf{Example} \\ \hline

				Key & This entry is composed by two parts: Project name and issue number. It was used to keep a link between projects under study and data gathered & PRJONE-001 \\ \hline

				Priority & Indicates the priority assigned to the issues by internal stakeholders after the evaluation phase of the sprint. A lower number means greater priority. & 3 \\ \hline

				Estimate time & Shows the effort estimation performed by the developers of the team & 2.5 \\ \hline

				Effective time & Shows the effective time it took to complete the issue from when the status was updated & 5 \\ \hline

				Opened on & Date and time of when the issue was opened. & Tue Jan 07 09:39:59 CET 2014 \\ \hline

				Updated on & Date and time of when the work on the issue started & Thu Jan 22 10:41:27 CET 2015  \\ \hline

				Closed on & Date and time of when the issue was closed & Wed Jan 21 14:58:25 CET 2015 \\ \hline

				Description & Field that contains description, summary and comments related to the issue. & The process pipeline break when... \\ \hline

			\end{tabu}
		\end{table}


\subsection{Analysis of The Repositories} \label{sec:analysis_of_the_repos}
The company uses \textit{Concurrent Versions System} extensively. At the time of writing their system contains 870 repositories of various size belonging to 20 different projects in a many-to-many relationship. The company also deployed two different tools to browse such repositories with ease: \textit{HgWeb} and \textit{Crucible + FishEye}. They are migrating to the latter since it also allows asynchronous code reviews and integrates seamlessly with the Issue Tracker and the Forum as part of the suite developed by Atlassian.

The analysis performed to this archival source was both manual and automated. The former served the purpose of finding and corroborate the preliminary thematic analysis performed to identify possible TD items. The latter, instead, was used to obtain trends about how said items evolve. Both methodology are thoroughly analyzed in following subsections.

\subsubsection{Manual analysis}
    This inquiry target the code comments found in the relevant repositories associated with the four cases under study. In fact, such entries could include TODOs or complaints about the code they are tied to. With this in mind the inquiry consisted in manually browsing the source code files of the five most recent versions available on the CVS. Once collected, duplicate comments have been removed (e.g.\ comments that did not change across different revisions) in order to not bias the relevance of them when performing the Thematic analysis.


\subsubsection{Automated analysis} \label{sec:automated_analysis}
    According to Staron et al.\ \cite{metrics_paper}, in order to perform an automatic analysis, a measurement system needs be defined. Therefore, after performing the preliminary thematic analysis, which highlighted possible TD items within testware, a \textit{specification of measurement} has been created for applicable items. Applicability, in this context, means that chosen metrics are 1) created by statically parsing the source code on a per revision base, and 2) the extrapolation of the data does not require tremendous computational power since inaccessible by author of this study. Number one is to follow the common practices discovered in the related work section, whereas number two is due to the lack of resources.

    Furthermore, Staron et al.\ \cite{metrics_paper} require to specify thresholds to identify optimal, acceptable, and not acceptable level for each metric. However, considering the exploratory nature of this study this action has been omitted. In fact, the existing body of knowledge is lacking prior examples of such thresholds or, if present, their use in relation with test code is novel. Therefore, said activity would have been based on the author best guesses: a fatal threat to validity.

    With that said, adhering to the guidelines provided by Staron et al.\ \cite{metrics_paper}, the following specification measurements has been created (Table \ref{tab:complexity_measurement_system}, \ref{tab:statements_measurement_system}, and \ref{tab:length_measurement_system} ) for three of the items revealed by the thematic analysis. Rationales regarding the infeasibility of the remaining two items are provided in chapter \ref{sec:study_results}.

    Finally, the tool that gathered such measurements is hosted as an open source repository on GitHub\footnote{\href{https://goo.gl/VWaTGm}{https://goo.gl/VWaTGm}}. Furthermore, appendix \ref{app:database} illustrates the database's schema that said tool generates and lists the queries used to calculate the metrics.

    \begin{table}[!htb]
		\centering
		\tabulinesep=1.2mm
		\begin{tabu} to \textwidth {|X|X[2]|}

			\hline
			\textbf{Entry} & \textbf{Action taken} \\
			\hline

			Information need & How is the code complexity evolving in reposiotry X? \\
			\hline

			Indicator & McCabe's \cite{cyclomatic_complexity} cyclomatic complexity \\
			\hline

			Analysis model & Not applicable \\
			\hline

			Derived measures & Not applicable \\
			\hline

			Measurement function & SUM(CC[number of revision] for all functions)/TNF[number of revision] \\
			\hline

			Base measures & Object: revision \newline
			                    \textbf{CC}: cyclomatic complexity of a given function \newline
			                    \textbf{TNF}: total number of function for a given revision \\
		    \hline

			Measurement method & For CC (summary of what explained in \cite{cyclomatic_complexity}):
			    \begin{enumerate}
			    \item Calculate number of edges in the function graph (NE)
			    \item Calculate the number of nodes in the function graph (NN)
			    \item Calculate the number of connected components in the function (NCC) graph
			    \item CC = NE - NN + 2 * NCC
			    \end{enumerate}

			For TNF:
			\begin{enumerate}
			\item TNF = Calculate the number of functions in a given revision
			\end{enumerate}

			\\
			\hline

		\end{tabu}
		\caption[Specification measurement for cyclomatic complexity]
        {Specification measurement for cyclomatic complexity following Staron et al.\ \cite{metrics_paper} guidelines}
        \label{tab:complexity_measurement_system}
    \end{table}

    \begin{table}[!htb]
		\centering
		\tabulinesep=1.2mm
		\begin{tabu} to \textwidth {|X|X[2]|}

			\hline
			\textbf{Entry} & \textbf{Action taken} \\
			\hline

			Information need & How is the statements' complexity evolving in reposiotry X? \\
			\hline

			Indicator & Percentage of complex statments \\
			\hline

			Analysis model & Not applicable \\
			\hline

			Derived measures & Not applicable \\
			\hline

			Measurement function & COUNT(CS[number of revision])/TNS[number of revision] \\
			\hline

			Base measures & Object: revision \newline
			                    \textbf{CS}: number of complex statments \newline
			                    \textbf{TNS}: total number of statements \\
		    \hline

			Measurement method & For CS:
			    \begin{enumerate}
			    \item count the number of statments with length greater than 80 characters (excluding whitespaces).
			    \end{enumerate}

			For TNS:
			\begin{enumerate}
			\item Calculate the number of statements in a given revision
			\end{enumerate}

			\\
			\hline


		\end{tabu}
		\caption[Specification measurement for statements' complexity]
        {Specification measurement for statements' complexity following Staron et al.\ \cite{metrics_paper} guidelines}
        \label{tab:statements_measurement_system}
    \end{table}

    \begin{table}[!htb]
		\centering
		\tabulinesep=1.2mm
        \begin{tabu} to \textwidth {|X|X[2]|}

			\hline
			\textbf{Entry} & \textbf{Action taken} \\
			\hline

			Information need & Is the code base in Repository X properly modularized? \\
			\hline

			Indicator & Average length of functions \\
			\hline

			Analysis model & Not applicable \\
			\hline

			Derived measures & Not applicable \\
			\hline

			Measurement function & SUM(FL[number of revision])/TFN[number of revision] \\
			\hline

			Base measures & Object: revision \newline
			                    \textbf{FL}: number of LOC \newline
			                    \textbf{TFN}: number of functions \\
		    \hline

			Measurement method & For FL:
			    \begin{enumerate}
			    \item count the number of LOC in a given function.
			    \end{enumerate}

			For TFN:
			\begin{enumerate}
			\item Calculate the number of functions in a given revision
			\end{enumerate}

			\\
			\hline


		\end{tabu}
		\caption[Specification measurement for single responsibility violation]
        {Specification measurement for single responsibility violation following Staron et al.\ \cite{metrics_paper} guidelines}
        \label{tab:length_measurement_system}
    \end{table}

\FloatBarrier

\subsection{Semi-Structured Interviews} \label{sec:semi-structured_interviews}
During this study a total of four semi-structured interviews was conducted; they follow the guidelines expressed in \cite{interview_guideline}. Two of them were used to validate the metrics defined in Section \ref{sec:automated_analysis} whereas the other two focused in validating the findings with a particular focus on \textbf{RQ 3}, due to the scarcity of data in such topic. Although the author is aware that findings supported by such a reduced sample cannot carry any statistical weight, the findings have been triangulated with few other sources to partially compensate. Furthermore, the study used the semi-structured interview format since it allows a sufficient degree of flexibility necessary to adapt the inquiry while maintaining the scope constrained.

The four interviewees are, at the time of writing, employees of the Company and actively worked on at least one of the four cases described in section \ref{sec:cases_definition}. Table \ref{tab:interviews_summary} summarizes the interviews conducted and in which projects they work. Moreover, all interviews were 1) conducted in a meeting room within the Company office with no external auditors and 2) lasted forty-five minutes.

\begin{table}[!htbp]
	\centering
	\tabulinesep=1.2mm
	\begin{tabu} to \textwidth {|X|X|X|}

		\hline
		\textbf{Interviewee ID} & \textbf{Projects contributed} & \textbf{Interview kind} \\
		\hline

		1 & All projects & Metrics validation \\
		\hline

		2 & Project A, B, and C & Metrics validation \\
		\hline

		3 & Project C & Findings validation \\
		\hline

		4 & Project A & Findings validation \\
		\hline

	\end{tabu}
	
	\caption[Summary of conducted interviews]{Interviews summary including goal of the interview and relevant projects for each interviewee as defined in section \ref{sec:cases_definition}.}
	\label{tab:interviews_summary}
\end{table}

The metrics validation interviews were conducted by showing, for each of the available metrics a sample of code real code before and after a thorough manual refactoring performed by the author. This decision has been taken to create a scenario similar to real life which is hard to replicate with dummy code created ad-hoc. The code samples were chosen in order to present interviewees with unfamiliar code base, since it could alter the results. However, for confidentiality reasons, the code used in such context cannot be included in this report.

Subsequently, each of the two interviewees had to rate on Likert's scale the possible maintainability efforts for each of those code blocks. Therefore, a higher results means a higher \textbf{perceived} maintainability effort.

Finally, findings validation interviews were conducted by asking how the identified TD items affects the maintainability efforts in a day to day work and which best practices are adopted (if any) to avoid an uncontrolled TD's growth. The questions for those interviews are reported in Appendix \ref{app:interview}.
%
%
% END DATA GATHERING
%
%


%
%
% START DATA INTERPRETATION
%
%
\section{Interpretation of the Data} \label{data_interpretation}

The interpretation of the data plays a key role in experimental studies like this one. Furthermore, analyzing both qualitative sources (e.g.\ forum) and quantitative ones (e.g.\ repositories) requires different strategies that account for differences in the nature of the data. Yin \cite{case_study_guide} lists and describes several methods for adequately perform this stage. However, some considerations are needed. Firstly, this Case Study is exploratory and draws from a scarce body of knowledge. Therefore, an inductive approach is more suitable. Secondly, while some data sources are suitable for a qualitative inquiry, others are not by nature. Therefore, the analysis must use at least two different methods to account for these problems. Finally, Yin \cite{case_study_guide} and Runeson et al.\ \cite{case_study_software_engineering}, strongly recommend to address rival theories.

For these reasons, qualitative data is analyzed through \textit{thematic analysis} \cite{qualitative_inquiry} allowing the inductive approach mentioned before. Quantitative source, instead, are scrutinized through \textit{pattern matching} \cite{case_study_guide}; according to Runeson et al.\ \cite{case_study_software_engineering} it is the most suitable approach to address rival theories.

Finally, the approach followed in this study relies on both methods concurrently. In fact, the inquiry used the coding practices in the first part of the study, aiming to extrapolate recurrent patterns and problems from different sources. Subsequently, through pattern matching a theory draft has been created which has been validated by semi-structured interviews followed by an ulterior refining phase of the previously created themes.

\subsection{Thematic analysis} \label{sec:thematic-analysis}
As Braun and Clarke \cite{thematic_analysis} state, thematic analysis, among other perks, \textit{``can usefully summarize key features of a large body of data, and/or offer a ‘thick description’ of the data set.''}. Furthermore, it is a flexible and relatively easy method to use \cite{thematic_analysis}.

However, to proper perform this kind of analysis, researchers have to go through several steps. Namely 1) familiarizing with the data, 2) generating initial codes, 3) searching for themes, 4) reviewing themes, 5) defining and naming themes, and 6) producing the report. Table \ref{tab:thematic_analysis_steps} explains how each one of these steps was performed. Moreover, said table includes a real example of how data has been used to generate the themes and codes listed in tables \ref{fig:rq1_sources} and \ref{fig:rq2_sources}.

In this study, the inquiry used two different thematic analysis. The first one, helped categorize data coming from 1) informal interviews and 2) mining of the archival records (i.e.\ forum, issue tracer, and backlogs). The second one was instead needed to extrapolate and categorize data obtained from the semi-structured interviews. In this latter case, considered the small number of subjects, no example of how the codes and themes have been produced is reported. This is a voluntary choice made to safeguard the anonimity of the interviewees.

\tabulinesep=1.2mm
\renewcommand{\arraystretch}{1.5}
\begin{longtabu} to \textwidth {|X[3]|X[8]|}

    \hline

    \textbf{Step} & \textbf{Action Taken} \\
    \hline

    Familiarizing with the data &
    The data generated has been continuously revised and, during collection phase, summarized with the help of NVivo\texttrademark\ 10 for Windows\texttrademark{}. Furthermore, data not needing any summary (e.g.\ coming from the issue tracker) was read at least twice. For instance, while summarizing the content of informal interviews the words \textit{``messy''}, \textit{``complex''}, \textit{``unreadable''} stand out. \\
    \hline

    Generating initial codes &
    As Braun and Clarke \cite{thematic_analysis} suggest, codes must be generated without constraining the boundaries; i.e.\ researchers must create as many codes as possible to avoid being bias by prior knowledge. The resulting set has been filtered in subsequent phases of the analysis. The three words listed at the previous step where categorized into five different codes: \textit{bad code}, \textit{lack of common guidelines}, \textit{poor skills}, \textit{unfamiliarity with UFT}, and \textit{unfamiliarity with the codebase}.
    \\
    \hline

    Searching for themes &
    At this stage the codes previously generated are grouped in coarse themes based on their likeness. No codes should be excluded at this step \cite{thematic_analysis}. Following the prior example, two themes were created: \textit{readability issues}, \textit{lack of skills}, and \textit{unfamiliarity}.
    \\
    \hline

    Reviewing themes &
    This step consists in polishing the themes generated until further modifications of the structure created adds nothing to the result \cite{thematic_analysis}. At this step, the \textit{lack of skill} theme was discarded and a global organizing theme has been introduced to separate these themes from others non reported in this example.
    \\
    \hline

    Defining and naming themes &
    At this stage themes are renamed in a way that is clear for both the researcher and the reader. In particular, theme names must increase traceability from data to the results and clearly state the contribution to the study. Themes that do not meet these criteria must be discarded or further rearranged \cite{thematic_analysis}. In this study, the themes have been arranged and the the words \textit{``messy''} and \textit{``complex''} have been included in the \textit{Complex functions} code, whereas \textit{``unreadable''} is part of the \textit{Complex statements} code.
    \\
    \hline

    Producing the report &
    Although the inquiry conducted in this study used the same approach to both organize the data extracted from the archival records and extrapolate validation data from the semi-structured interviews, this stage differs between the two operations. In the former case, the thematic map was combined with the frequency of codes to determine most relevant aspects of Technical Debt that arise in testware; such results are visible in Fig.\ \ref{fig:rq1_sources} and \ref{fig:rq2_sources}. Furthermore, the final set of codes generated is reported in tables \ref{tab:themes_rq1} and \ref{tab:themes_rq2} Therefore, a report was not generated. In the latter case, instead, the thematic map was used to generate narratives that have been anonymized and included in Section \ref{sec:discussion}.
    \\
    \hline

\caption{Description of steps taken while performing thematic analysis with example of the procedure has been used with real data.}
\label{tab:thematic_analysis_steps}
\end{longtabu}



\subsection{Pattern matching} \label{sec:pattern-matching}
Yin \cite{case_study_guide}, Runeson et al.\ \cite{case_study_software_engineering}, and Trochim \cite{pattern_matching} consider Pattern matching as one of the best approaches to meld together existing theories with the data gathered within a study. Furthermore, it eases the evaluation of rival theories since a it allows to rule them out thanks to the fitness with the discovered pattern \cite{pattern_matching}.

In this Thesis, the pattern was extrapolated from the Repositories and matched against quantitative and qualitative models provided by the literature. For obvious reasons, in the latter case the inference conducted has been solely inductive, meaning that the findings associated are not corroborated by statistical weight and hence might lack external validity.

Trochim \cite{pattern_matching} highlights several pitfalls that must be considered when using this technique; otherwise the power of this method is greatly hindered.

Firstly, the conceptualization of the model must be appropriate, e.g.\ using formulae to describe a purely qualitative model is discouraged since constants and factors derive from the researcher best guesses.

Secondly, the level of generalization of the formulated pattern must match the purpose of the study and the desired level of generalization. In this Thesis, given the exploratory nature findings are not intended to describe in detail real phenomenon, but, instead, aim to highlight a gap in the knowledge which requires more in-depth analysis.

Third, pattern formulation tend to be more effective when performed without prior knowledge of the observed pattern. In fact, the researcher judgment could be biased by this prior knowledge. Fact that inflates the threats to reliability and internal validity.

Next, the holistic perspective that researchers tend to use while performing pattern matching could exclude relevant details. This specially applies when pattern are extracted from a set of metrics or measures. Moreover, when measuring the fitness of patterns using statistical methods, researchers should consider their analytical accuracy with respect to the context in which are embedded.

In this study the trends shown in figures \ref{fig:avg-complexity-together}, \ref{fig:avg-length-together}, and \ref{fig:avg-complexity-together} have been matched with known patterns identified in the literature. An example of this is the comparison of \textit{Function complexity} trends with the model reported by Martini et al.\ \cite{martini2014architecture}, discussed in section \ref{sec:disc-function-complexity}. In this case the trends generated by statically analyzing the source code repositories have been qualitatively assessed against said model and the discussion provides narratives explaining the differences between the obtained results with the one provided in the other study.


\section{Execution}
This section describe how the study has been executed. It includes the list of macro phases used to gather, analyze, synthesize, and verify data.
Figure \ref{fig:study-phases} shows the chronological succession of steps followed.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=\textwidth]{figure/methods/steps.png}
    \caption{Study phases in chronological order}
    \label{fig:study-phases}
\end{figure}

During contextual analysis, the goal was to familiarize about the context, the awareness of the Company's employees towards technical debt.
Therefore, the initial phase consisted in mining the forum, the meta-documentation and perform the mentioned informal interviews. This stage was necessary to constrain the inquiry and focus on the perceived problems. For instance, the meta-documentation analysis highlighted a lack of guidelines that, once inspected through informal interviews, seemed to cause code reusability issues among different repositories and projects. Furthermore, at this stage a first draft of the metrics was crafted and subsequently validated through the two semi-structured interviews. The final result of such validation is available in tables \ref{tab:complexity_measurement_system}, \ref{tab:statements_measurement_system}, and \ref{tab:length_measurement_system}.

The second phase was a focused analysis of the remaining data sources. The forum and the issue trackers confirmed the problems tied with the lack of guidelines which were partially confirmed by the informal interviews. In fact, in a discussion thread a tester was explicitly asking for \textit{``help in understanding what this code... is doing''}. Moreover, the issue tracker included \textit{chores} related to code hygiene; e.g.\ \textit{``split functions X,Y and Z because they contain repeated logic.''} (X,Y, and Z are placeholders for the real functions' names). Once this step was completed and the proposed metrics have been validated, both manual and automatic analysis of the code-base was performed.

The third phase consisted in conducting the synthesis of all the data collected through thematic analysis and pattern matching, which eliminated and explained parts of the rival theories. Example of how this step was performed are reported in sections \ref{sec:thematic-analysis} and \ref{sec:pattern-matching}.

Fourthly, the refined set of codes was validated through two semi-structured interviews. The interview template is included in Appendix \ref{app:interview}.

As a final note, the writing of this report has been carried out concurrently with the four steps hereby described.
