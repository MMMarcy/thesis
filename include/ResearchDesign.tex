\chapter{Methods} \label{methods}
This section describes the processes used within this study. However, related Validity Threats are discussed in \ref{validity_threats}.

\section{Case Study Design}

\section{Data Gathering}
As recommended by Yin \cite{case_study_guide}, I gathered data from various sources in order to increase Triangulation and hence improve the study's reliability. Namely 1) Informal Interviews (a.k.a.\ Coffe Machine / Water Bowl Interviews), 2) Mining of Comapany's Forum, 3) Mining of Comapny's Issue Tracker, 4) Mining of Repositories, and 5) Semi-structured Interviews.

However, some of the data sources were used as secondary sources, meaning that they assited the inquiry conducted by other methods. I.e.\ I used Informal Interviews, and the mining of the forum mainly during the first phase of this Exploratory Case Study for narrowing the scope. Consequently, I achieved a more focused analysis of other sources (e.g.\ Analysis of the repositories).

Finally, considering the extensive amount of data that has been generated, I won't directly include it in this Report. Instead, it is available online at \href{http://somthing/}. However, readers should be aware that the raw Data was anonymized and every sensitive information was also removed. \todo{Insert link to thesis database}

\subsection{Informal Interviews}
I used this Data Source to overcome resource's scarcity. In fact, I had to limit the number of Formal Interviews (see \ref{semi-structured_interviews}) and hence I opted for this source. However, considering their non-reproducible nature I decided to not use them for sensitive parts of the study like Theory Validation. \tocite{Reference a peper on such interviews}

The method used for each one of these Interviews varied depending of several factors. Firstly, the frequency of such Interviews was random. They occurred during lunch and/or coffee breaks. The settings also varied both in terms of interviewee and location. However, most of them occurred in the coffee room that the company provides to its employees. Furthermore, the topic covered depended on my current insight. In fact, they helped me formulate a theory draft in a \textit{trial and error} fashion. For instance, at the beginning my questions were about documentation and similar artifacts. Afterwards the scope narrowed with questions targeting current problems and solutions related to test maintenance. These last questions allowed me to discover critical time frames that were also reflected in the Company's Repositories.

As a final note, due to the randomness of both subjects, locations, and frequency, I was unable to create proper artifacts for these interviews. However, the Thesis Database contains brief descriptions of the topics covered by each one of these.



% Frequent
% Different Settings
% Tipical Questions Asked
% Focused on the current problem (trial and error method)
% Available on the Thesis Database
% 



\subsection{Mining of The Forum}
In order to enable Knowledge Sharing the company deployed a Collaborative Software / Fourm. The one in use is Confluence, developed and maintained by Atlassian. In it both Technical and Non-Technical Employers create pages for sharing procedures, ideas, or create non synchronous discussion threads about problems. For these reasons this source is valuable since it contains numerous informations, which can shed light on the problems this Thesis aim to study.

However, considering the magnitude of the data included I decided to follow a systematic approach in order to ensure that no relevant information was missed. In fact, the system contains data covering several years and include several thousands discussions threads and many more answers. To overcome this probelm I used the embedded search enginge. The Strings entered were included withing double quotes in order to enable the exact match feature. This choice was needed to limit the number of results. For instance, \textit{test maintenance} yielded 8467 results whereas \textit{"test maintenance"} 28. Furthermore, this reduced result set has gone through two filtering steps before being included.

The first coarser filter is based on a reduced time frame. I eliminated hits older than 1 year for several reasons. Firstly, the Testware stack has been cahnged during last year (2014) and hence my findings could be biased by information not relevant or dependant to the old technologies. Secondly, wehn deciding an adequate time span I considered the mortality of the resources. In fact, the Company greatly relies on consultants and their turnover is shorter than normal employees.

The second finer filter is based on the relevance. In fact, the mentioned search engine uses a full-text search and hence results are returned even if someone commented the original post mentioning the considere keywords. Therefore, I decided to eliminate every hit that did't contain the keywords in the title or in the body of the original post. However, if the answers mentioning the keyword were referring to another entry/issue or esplicitly introducing the topic summarized by the keyword I included them.

Finally, the remaining set of posts has been carefully read and pertinent ones has been anonymized and transcribed in the Thesis database. Table \ref{tab:confluenceMiningResults} the results obtained for each step for the keywords used.

\input{table/confluenceMiningResults}

\subsection{Mining of The Issue Tracker / Backlogs}

The Company uses an all in one solution for managing issues and their Agile Environments: JIRA + Agile Plugin. These solutions are also managed by Atlassian. Given the vastitiy of items in the database the assumptions I made for the Confluence Forum still apply. Furthermore, also the filtering procedure is similar. However, I attributed more importance to this source.

In fact, both issues and User Stories are one of the aforementioned maintenance triggering factors. Furthermore, it is possible to track such items to specific commits included in the repositories. Therefore, this source allows a high degree of traceability and it can be used to consolidate some of my insights.

The filtering process differs due to the different data formats used by reporting User Stories and Issues. In fact, it is more concise and hence the probability of a hit by keyword is lower. Therefore I decided to include in my search the related issues of each result in order to compensate.

However, due to the highly sensitive informations about issues, bugs, and planned features included in this source I cannot disclose any part of them and hence I limit to report the the keywords used with the raw numbers.

\todo{Make table/chart/appendix with links between issues related to testing and repositories commits}

\input{table/jiraMiningResults}


\subsection{Analysis of The Repositories}

\subsection{Semi-Structured Interviews} \label{semi-structured_interviews}

\section{Interpretation of the Data} \label{data_interpretation}








\section{Literature Review Process} \label{literature_review_process}
The literature reviewed in this section comes from a systematic approach used to ensure that all relevant studies were considered during the review phase.

The process used consists in these six steps:
\begin{enumerate}
    \item Identification of secondary studies on the topic.\\
        This first stage of the review revealed that secondary studies on the topic are scarce. The search in the main databases for the field returned three hits: 1) a Sistematic Literature Review, 2) a complementary (with respect to 1) Multivocal Literature Review, and 3) a Systematic Mapping Study of Technical Debt.
        
    \item First level of snowballing and filtering by meta-data.\\
        From the analyzed studies in 1 I firstly considered the ones that in authors' opinion are not relevant for this Thesis. Specifically the ones targeting Testing Technical Debt, Source Code Technical Debt, or both. After this first coarse filter I eliminated the ones not relevant based on meta-data analysis (Title and Abstract). The remaining ones have been added to the list used during quality and relevance assessment. 
        
    \item Extension with main databases and filtering by meta-data.\\
        The main databases used for extending this literature review are: IEEE Xplore, Springer Verlink, Science Direct, and ACM. The keywords inserted were "Technical Debt" and "Testing Maintenance". The filtering process is analogous to the one used in point 2.
    
    
    \item Second level of snowballing and filtering by meta-data.\\
        From the studies remaining after point 2 and 3 I performed another round of snowballing. After that the papers included has been filtered with the same procedure used in 2 and 3.
    
    
    \item Assessing quality and relevance of studies.\\
        Finally, the list of studies remained have been analyzed throughoutly. The metrics used for evaluating the studies are relevance to this Thesis and intrisic quality of the study itself. 
    
    
    \item Inclusion.\\
        The studies left after step 5 were included in the literature review
    
\end{enumerate}





