\chapter{Related work} \label{related_work}

The reviewed related work presented in this section only targets aspects relevant to the goals of this Thesis \ref{thesis_goals}. Specifically the main focus is available research targeting Testing Technical Debt. However, in order to accurately assess the impact of Testing Technical Debt related to test code internal quality I also reviewed studies that 1) define and identify and quantify Source Code TD \cite{mapping_study_td}. A special emphasys has been put to studies that create a link between the two dimensions since they represent possible rival theories and, hence, need a more in-depth analysis \cite{case_study_guide}. The last part of the chapter summarizes how this brief literature review has been conducted.

\section{Testing Technical Debt} \label{testing_td}

The term Testing Technical Debt (TDD) refers to the TD that accrues in the context of \textit{testware}, meaning an umbrella term to identify artifacts created and / or used while running tests. Another source of TDD is the processes in use when executing the testware. As indicated by \cite{mapping_study_td, exploration_of_td, exploration_of_td2} there is a relevant number of sources targeting TTD, but they focus mainly on lack of test automation, low code coverage, and lack of tests.

For instance \cite{test_automation_td} founds a possible source of Technical Debt in problems related to \textit{Test Automation}. As result of their Case Study they report four findings that cause testing technical debt to accrue. Namely: 1) \textit{Reuse and sharing of test tools brings issues that need to be considered}, 2) \textit{Test facility infrastructure is not transparent and may alter the test results if not accounted for}, 3) \textit{Generalist engineers expect that their tools are easy to use}, and 4) \textit{Accepted development practices for test code are potentially less rigorous than for production code}. As stated before, all of these use a environmental point of view as TD enabling factor. Also number 4, which seem to target the intrinsic nature of test code, focuses on missing documentation (relative to test scripts) and quick hacks that are dependent on the system architecture and, hence, related to such TD dimension. Shah et. al.\ \cite{exploratorying_testing_td} studied manual testing as source of Technical Debt. Precisely Exploratory Testing. This technique, has the potential to greatly inflate the accumulation of technical debt since is, by nature, not reproduceable and, hence, not suitable for regression testing. The interest matured in using only this technique as mean of Quality Assurance is extremely high. Brown et. al.\ \cite{td_current_vs_optimal_quality} consider not carrying out a test-plan the root cause of TTD since defects can remain and, hence, propagate to production environments. Once there, the reparation cost is extremely high both in terms of resources and brand damage. Therefore this TDD sub-item has the highest interest.

As it is possible to evince form 1) previous paragraph and 2) \ref{literature_review_process}, studies targeting the intrinsic nature of Test Code are scarce, especially the ones framing the problem within the Technical Debt framework. However, a number of studies report a common fragility of tests with respect to maintainability. For instance, Chen and Wang \cite{gui_scripts_bad_smells} identified 11 \textit{Bad Smells} in GUI test scripts. Nine of them map to known source code bad smells, whereas two of them apply to GUI test scripts only. The authors propose a total of 16 refactoring patterns which will lessen the amount of time spent on future maintenance activities. Moreover, they proved their claim through two experiments that showed that 1) by looking for bad smells, the potential problems in a test script can be identified; 2) bad smells do exist in test scripts; and 3) the refactoring methods proposed in their study are useful to eliminate Bad Smells. However, the paper don't indicate the sample size used by such experiments and, hence, is impossible to evaluate the experiments' reliability. Another study that report a link between test code and software code is \cite{pitfalls_in_introducing_regression_testing}. This experience report lists a total of 34 \textit{common pitfalls} that companies introducing test automation should avoid. Among them there are two which are strictly related to the internal quality of test code. Namely \textit{The testware is not handled with the same care and professionalism as the shipped code - ignoring that test automation is software development}, and \textit{Lack of test development guidelines thus, endangering reusability, repeatability and maintainability of testware}. However, the consequences in terms of maintenance impact of such pitfalls is not reported. Finally, Leotta et al.\ \cite{pageObjectPattern} performed a case study in which a structured approach to Selenium tests, i.e.\ Page Object Pattern, allowed a significant boost in maintainability of such tests. In their approach an additional layer of indirection between web pages loosened the coupling between tests and targeted components. This, in turn, led to remarkable results in maintenance. The study shows that Object Oriented design is applicable to GUI tests and improves maintainability in such context in a similar way to source code.

Summarizing, both Academia and Industry aknowledged the phenomenon of Techincal Debt in Testing Environments. However, there is a reclutance in both worlds when it comes to consider test code as important as software source code. Furthermore, the literature reviews in this Thesis don't include any study showing the implications of such approach within the Technical Debt Framework.

\section{Source Code Technical Debt} \label{source_code_technical_debt}
This section summarizes reviewed relevant studies that target identification, quantification and management of TD items that inflate source code's decay rate over time. Given the exploratory nature of this study I won't specifically adopt any of them, but instead base my data inspections on common elements that are found within this literature.

After inspecting relevant papers as indicated in \ref{literature_review_process} is clear that researchers consider the panacea of code smells \cite{code_smell_definition} the main cause of code decay \cite{mapping_study_td}. For instance, \cite{domain_specific_code_smells} studies the suitability of the original definition of code smells in the context of domain-specific systems. The study reveals that only a small amount of tailoring is needed to adapt the original definitions. This finding imply that the concept is code smell is easily adaptable to other contextes. Similarly, Eisenberg \cite{threshold_approach_to_td} quantifies source code technical debt with both static and dynamic metrics applied to the code-base. The static metrics measure the presence of code-smells. After this assessment phase his approach quantifies the total amount of Technical Debt in terms of resources needed to lower such amount within accpetable levels. These levels, together with the cost per occurrence, differ between sub-items. E.g. removing a duplicate code block is four times cheaper than splitting a complex class due to dependencies.

%TODO: conclusion of the section

\section{Literature Review Process} \label{literature_review_process}
The literature reviewed in this section comes from a systematic approach used to ensure that all relevant studies were considered during the review phase.

The process used consists in these four steps:
\begin{enumerate}
    \item Identification of secondary studies on the topic.\\
        This first stage of the review revealed that secondary studies on the topic are scarce. The search in the main databases for the field returned three hits: 1) a Sistematic Literature Review, 2) a complementary (with respect to 1) Multivocal Literature Review, and 3) a Systematic Mapping Study of Technical Debt.
        
    \item First level of snowballing and filtering by meta-data.\\
        From the analyzed studies in 1 I firstly considered the ones that in authors' opinion are not relevant for this Thesis. Specifically the ones targeting Testing Technical Debt, Source Code Technical Debt, or both. After this first coarse filter I eliminated the ones not relevant based on meta-data analysis (Title and Abstract). The remaining ones have been added to the list used during quality and relevance assessment. 
        
    \item Extension with main databases and filtering by meta-data.\\
        The main databases used for extending this literature review are: IEEE Xplore, Springer Verlink, Science Direct, and ACM. The keywords inserted were "Technical Debt" and "Testing Maintenance". The filtering process is analogous to the one used in point 2.
    
    
    \item Second level of snowballing and filtering by meta-data.\\
        From the studies remaining after point 2 and 3 I performed another round of snowballing. After that the papers included has been filtered with the same procedure used in 2 and 3.
    
    
    \item Assessing quality and relevance of studies.\\
        Finally, the list of studies remained have been analyzed throughoutly. The metrics used for evaluating the studies are relevance to this Thesis and intrisic quality of the study itself. 
    
    
    \item Inclusion.\\
        The studies left after step 5 were included in the literature review
    
\end{enumerate}


