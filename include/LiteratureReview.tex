\chapter{Related work} \label{sec:related_work}

The main focus of this brief literature review is to inform the reader about available research targeting Testing Technical Debt (TTD). However, in order to accurately assess the impact of TTD with respect to test code internal quality also studies that define and identify and quantify source code TD (SCTD) \cite{mapping_study_td} were reviewed. A special emphasis has been put to studies that create a link between the two dimensions since they represent possible rival theories and, hence, need a more in-depth analysis \cite{case_study_guide}. Consequently, to be able to answer \textbf{RQ3}, relevant studies proposing models for Interest's quantification have been reviewed.

\section{Testing Technical Debt} \label{testing_td}

The term Testing Technical Debt (TTD) refers to the techinal debt that accrues in the context of \textit{testware}, meaning an umbrella term to identify artifacts created and / or used while running tests. Another source of TTD is the processes in use when executing the testware. As indicated by \cite{mapping_study_td, exploration_of_td, exploration_of_td2} there is a relevant number of sources targeting TTD, but they focus mainly on lack of test automation, low code coverage, and lack of tests.

For instance Wiklund et al. \cite{test_automation_td} found a possible source of TD in problems related to \textit{test automation}. As result of their case study they report four findings that cause TTD to accrue. Namely: 1) \textit{reuse and sharing of test tools brings issues that need to be considered}, 2) \textit{test facility infrastructure is not transparent and may alter the test results if not accounted for}, 3) \textit{generalist engineers expect that their tools are easy to use}, and 4) \textit{accepted development practices for test code are potentially less rigorous than for production code}. As stated before, all of these use a environmental point of view as TD enabling factor. Also number 4, which seem to target the intrinsic nature of test code, focuses on missing documentation (relative to test scripts) and quick hacks that are dependent on the system architecture and, hence, related to such TD dimension. 

Shah et al.\ \cite{exploratorying_testing_td} studied manual testing as source of technical debt. Precisely exploratory testing. This technique, has the potential to greatly inflate the accumulation of technical debt since is, by nature, not reproducible and consequently not suitable for regression testing. They claim that the interest matured in using only this technique as mean of quality assurance is extremely high. 

Similarly, Brown et. al.\ \cite{td_current_vs_optimal_quality} consider not carrying out a test-plan the root cause of TTD since defects can remain and propagate to production environments. Once there, the reparation cost is extremely high both in terms of resources and brand damage. Therefore, they state that this TTD sub-item has the highest interest among all testing technical debt ones.

To the best of the author's knowledge, studies targeting the internal quality of Test Code are scarce, especially the ones framing the problem within available technical debt frameworks. However, a number of studies report a common fragility of tests with respect to maintainability. 

For instance, Chen and Wang \cite{gui_scripts_bad_smells} identified 11 \textit{Bad Smells} in GUI test scripts. Nine of them map to known source code bad smells, whereas two of them apply to GUI test scripts only. The authors propose a total of 16 refactoring patterns which will lessen the amount of time spent on future maintenance activities. Moreover, they proved their claim through two experiments that showed that 1) by looking for bad smells, the potential problems in a test script can be identified; 2) bad smells do exist in test scripts; and 3) the refactoring methods proposed in their study are useful to eliminate Bad Smells. However, the article doesn't indicate the sample size used by the experiments and, hence, is impossible to evaluate the reliability. 

Another study that reports a link between test code and software code is \cite{pitfalls_in_introducing_regression_testing}. This experience report lists a total of 34 \textit{common pitfalls} that companies adopting test automation should avoid. Among them, there are two which are strictly related to the internal quality of test code. Namely \textit{the testware is not handled with the same care and professionalism as the shipped code - ignoring that test automation is software development}, and \textit{Lack of test development guidelines thus, endangering reusability, repeatability and maintainability of testware}. However, the consequences in terms of maintenance impact of such pitfalls is not analyzed. 

Leotta et al.\ \cite{pageObjectPattern} performed a case study in which a structured approach to Selenium tests, i.e.\ Page Object Pattern, allowed a significant boost in maintainability of such tests. This pattern adds an additional layer of indirection between web pages which loosens the coupling between tests and targeted components. This, in turn, led to remarkable results in maintenance. The study shows that Object Oriented design is applicable to GUI tests and improves maintainability in such context in a similar way to source code.

Summarizing, both Academia and Industry acknowledged the phenomenon of Technical Debt in Testing environments and artifacts. However, there is a reluctance in both worlds to consider test code as important as software source code.

\section{Source Code Technical Debt} \label{source_code_technical_debt}

This section summarizes relevant studies that target identification, quantification and management of TD items that inflate the required source code maintenance efforts over time. Given the exploratory nature of this study it won't specifically adopt any of them during theory building, but instead base data inspections on common elements that are found within this literature.

After inspecting relevant papers is clear that researchers consider the set of code smells \cite{code_smell_definition} the main cause of source code technical debt (SCTD) \cite{mapping_study_td}. For instance, \cite{domain_specific_code_smells} studies the suitability of the original definition of code smells in the context of domain-specific systems. The study reveals that only a small amount of tailoring is needed to adapt the original definitions. This finding imply that the concept is code smell is easily adaptable to other contexts. 

Similarly, Eisenberg \cite{threshold_approach_to_td} quantifies source code technical debt with both static and dynamic metrics applied to the code-base. The static metrics measure the presence of code smells. After this assessment phase his approach quantifies the total amount of TD in terms of resources needed to lower it within acceptable levels. These levels, together with the cost per occurrence, differ between sub-items; e.g.\ removing a duplicate code block is four times less demanding than splitting a complex class due to dependencies. These relative costs are extrapolated from the SONAR \cite{sonar_evaluate_td} framework.

Nugroho et. al.\ \cite{technicalDebtInterest} proposed a mathematical framework for quantifying technical debt with respect to source code. Their approach consists in assessing the quality of the code base through Cyclomatic Complexity \cite{cyclomatic_complexity} and dividing the code based in areas based on the complexity index. The following step consists in assessing the code base quality in a scale from 1 to 5 based on the percentage of low/medium/high risk code identified in precedence. Once this assessment phase is complete it is possible to estimate the necessary rework needed to reach the desired level of quality. For instance, increasing the code base quality from level one to level two require the refactoring of 60\% of the code base. With this data is possible to roughly estimate the Refactoring Effort in terms of man-months.

Marinescu \cite{assessing_technical_debt_eclipse} conducted a longitudinal study on two Java projects belonging to the Eclipse ecosystem. For each of these the author analyzed the technical debt's variation over a time span of 11 years covering more than 30 releases. The method used in this study for quantifying TD is based on static code analysis targeting eighth common bad practices. However, the study doesn't consider the interest related with such flaws and hence does not provide any indication about their impact.

A different approach for quantifying SCTD is proposed by Singh et al.\ \cite{code_td_comprehension_activities}. They developed a framework for estimating interest on technical debt based on code comprehension activities performed by developers. Those tend to spend more time in such activities when facing classes and functions that contain a higher amount of technical debt. This contribution clearly links the results provided by static code analysis with a greater effort in maintain code which is affected by technical debt.

Concluding, it is clear from the reviewed studies that quantification of TD with respect to source code quality is achieved directly or indirectly through static code analysis. This fact was considered whenever designing the data-gathering phase of this study.

%TODO: conclusion of the section

\section{Interest Quantification}

This section reviews studies that propose models that estimates the impact of technical debt interest and focuses on source code. It is possible to notice that there is an overlap with Section \ref{source_code_technical_debt}, but is intended. The author preferred analyzing these two aspects separately to facilitate readers in extrapolating different information from such studies.

Nugroho et al.\ \cite{technicalDebtInterest} propose a mathematical method to estimate the interest (maintenance effort). The index they propose is directly proportional with the current level of quality and inversely proportional with the product of the amount of refactoring needed and the rebuild value. Their model accounts for different languages productivity indexes and justify it saying that a system with higher quality is easier to maintain. However this model does not take into account the importance that different software components have. For instance, an Object tightly coupled with many others requires extra effort than a loosely coupled one. Therefore, the model predictions tend to flatten the resources needed, but it is useful for setting an effort baseline while assessing refactoring activities.

Brown et al.\ \cite{td_current_vs_optimal_quality} propose a high level definition of TD interest and its probability. Respectively: \textit{the probability that a particular type of Technical Debt will in fact have visible consequences}, and \textit{the added cost of performing maintenance on the part of the system that contains technical debt}. They argue that these definitions are the most generic ones from which specific ones that target particular debt-dimensions can be created.

Zazworka et al.\ \cite{4_methods_to_identify_td} studied the effectiveness of four different methods for Technical Debt identification on 13 versions of Apache Hadoop. Namely: code smells, automatic static analysis (ASA) issues, grime
buildup, and modularity violations. Interestingly, the study shows no correlation between any of the approaches meaning that technical debt identification procedures must be tailored accordingly.

This brief review of models show that despite having access to quantitative models for estimating the interest, they are still incomplete. Furthermore, there are studies suggesting that an holistic model is not viable, given the complexity of software systems. For this reason shedding light on Testing Technical Debt will help creating a specific framework to better estimate this source of TD.
